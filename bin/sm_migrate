#!/usr/bin/env python

# stdlib imports
import argparse
import os.path
import sys
import re
import glob
import logging
import tarfile
from xml.dom import minidom
import json

# third party imports
import pytz
from lxml import etree
from configobj import ConfigObj

# local imports
from shakemap.utils.config import get_config_paths, get_configspec
from shakemap.utils.utils import get_network_name, migrate_gmpe, set_gmpe
from shakemap.utils.amps import AmplitudeHandler
from shakelib.rupture.origin import read_event_file, write_event_file, Origin
from shakelib.rupture.factory import text_to_json, rupture_from_dict_and_origin
from impactutils.time.ancient_time import HistoricTime
import shakemap.utils.queue as queue
from shakemap.coremods import transfer

KM2SEC = 3600.0/111  # seconds per kilometer
TIMEFMT = '%Y-%m-%dT%H:%M:%SZ'


def get_parser():
    description = '''Migrate existing ShakeMap 3.5 data directories.

    This program takes one required file, a tarball of one or more ShakeMap 3.5
    event directories.  To create this file, run a command like this:

    find . -name "*_dat.xml" -o -name "*_fault.txt" -o -name "source.txt" -o -name "info.json" -o -name "*.conf" | tar -czvf ~/sm35_inputs.tgz -T -

    This program takes a subset of the ShakeMap 3.5 input data and the info.json file
    and uses those files to create the corresponding ShakeMap 4.0 input data.

    Files used:
      - *_dat.xml data files in XML format. These are unchanged in ShakeMap 4.0.
      - *_fault.txt fault files in text format. These will be converted to GeoJSON.
      - source.txt Text file possibly containing mechanism and ??
      - info.json Metadata file from which we extract:
        - Origin information for new event.xml file.
        - bias settings (see -s option) 

      - *.conf Config files, of which generally speaking only grind.conf is supported:
               - IPEs are currently not supported.
               - A smaller subset of GMICE are supported in ShakeMap 4.0 at the time of this writing.
               - GMPE selections will be converted to the closest matching GEM equivalent (see -m).
               - outlier max_deviation and max_mag values will be converted.
               - 

    '''
    parser = argparse.ArgumentParser(description=description,
                                     formatter_class=argparse.RawTextHelpFormatter)
    parser.add_argument('tarfile',
                        help='Input root data tarfile')
    hlptxt = '''Ignore directories where event_source (from model.conf)
does not prepend eventid'''
    parser.add_argument('-i', '--ignore-naked-ids', action='store_true', default=False,
                        help=hlptxt)
    hlptxt2 = '''Ignore previous bounds/resolution settings discovered
in input data.'''
    parser.add_argument('-b', '--skip-bounds', action='store_true', default=False,
                        help=hlptxt2)
    parser.add_argument('-s', '--skip-settings', action='store_true', default=False,
                        help=hlptxt2)
    parser.add_argument('-m', '--migrate-file',
                        help='Supply custom migrate.conf')
    return parser


def _extract_file(member, tarball, filename, event_dir):
    datafile_object = tarball.extractfile(member)
    data = datafile_object.read().decode('utf-8')
    datafile_object.close()
    datafile = os.path.join(event_dir, filename)
    with open(datafile, 'wt') as dfile:
        dfile.write(data)


def main(args):
    install_path, data_path = get_config_paths()
    if not os.path.isdir(data_path):
        print('%s is not a valid directory.' % data_path)
        sys.exit(1)

    # get the system model.conf
    modelfile = os.path.join(install_path, 'config', 'model.conf')
    model_config = ConfigObj(modelfile)
    netid = model_config['system']['source_network']

    # load the migrate.conf file specified by user, if any
    if args.migrate_file:
        migrate_conf = ConfigObj(args.migrate_file)
    else:
        migrate_conf = None

    # get the system modules.conf file, which tells us about implemented
    # GMICE
    modulefile = os.path.join(install_path, 'config', 'modules.conf')
    module_conf = ConfigObj(modulefile)

    # We're going to be inserting these events in the database later
    handler = AmplitudeHandler(install_path, data_path)

    alleventids = []
    networks = {}

    # accumulate a list of errors so we can print them all at the end.
    errors = []
    conversions = {}
    print('Processing events...')

    # loop over events in the input tarball
    tarball = tarfile.open(args.tarfile, mode='r:gz')
    for member in tarball.getnames():
        if 'save' in member:
            continue
        parts = member.split('/')
        filename = parts[-1]
        eventid = parts[-3]
        if eventid not in alleventids:
            # print('Processing event %s...' % eventid)
            sys.stdout.write('.')
            sys.stdout.flush()
            alleventids.append(eventid)

        if not eventid.startswith(netid):
            if args.ignore_naked_ids:
                # print('Skipping naked ID %s.' % eventid)
                continue
            else:
                eventid = netid + eventid

        event_dir = os.path.join(data_path, eventid, 'current')
        if not os.path.isdir(event_dir):
            event_dir = create_event_directory(eventid, data_path)

        if filename.endswith('.conf'):
            cfg_errors, tconversions = parse_config(
                event_dir, tarball, member, migrate_conf=migrate_conf)
            errors += cfg_errors
            conversions.update(tconversions)
        elif filename.endswith('_dat.xml'):
            _extract_file(member, tarball, filename, event_dir)
        elif filename.endswith('_fault.txt'):
            _extract_file(member, tarball, filename, event_dir)
        elif filename in ['info.json']:
            if 'download' in member:
                continue
            edict, info_errors, info_conversions = parse_info(event_dir, tarball, member,
                                            netid, networks, module_conf,
                                            skip_bounds=args.skip_bounds,
                                            skip_settings=args.skip_settings)
            errors += info_errors
            conversions.update(info_conversions)
            handler.insertEvent(edict)
        elif filename == 'source.txt':
            _extract_file(member, tarball, filename, event_dir)
        else:
            errors.append('Unknown input file %s for event %s' %
                          (filename, eventid))

    tarball.close()
    sys.stdout.write('\n')

    print('%i event directories written.' % len(alleventids))
    print('The following errors or warnings were detected:')
    for error in errors:
        print('\t"%s"' % error)

    print('The following GMPE conversions were applied:')
    for old_gmpe, gmpe_tuple in conversions.items():
        new_gmpe, ref = gmpe_tuple
        print('\t"%s" (%s) was converted to new GMPE "%s".' %
              (old_gmpe, ref, new_gmpe))

    # go back through the data, convert fault.txt files into json rupture format,
    # also make sure we have all required components
    for eventid in alleventids:
        event_dir = os.path.join(data_path, eventid, 'current')
        make_rupture(event_dir)


def make_rupture(event_dir):
    fault_files = glob.glob(os.path.join(event_dir, '*_fault.txt'))
    if not len(fault_files):
        return
    fault_file = fault_files[0]
    xmlfile = os.path.join(event_dir, 'event.xml')
    eqdict = read_event_file(xmlfile)
    jdict = text_to_json(fault_file, new_format=False)
    origin = Origin(eqdict)
    try:
        rupt = rupture_from_dict_and_origin(jdict, origin)
    except Exception as e:
        x = 1
    _, ffile = os.path.split(fault_file)
    fbase, _ = os.path.splitext(ffile)
    jsonfile = os.path.join(event_dir, fbase+'.geojson')
    rupt.writeGeoJson(jsonfile)
    for ffile in fault_files:
        os.remove(ffile)


def parse_transfer(event_dir, tarball, member):
    parts = member.split('/')
    eventid = parts[-3]
    grindfile = tarball.extractfile(member)
    lines = grindfile.readlines()
    for line in lines:
        if line.strip*().startswith('#'):
            continue
        if not len(line.strip()):
            continue
        if 'pdl_config' in line:
            # this applies to the installation at NEIC
            # more knowledge of regional SM installations is required
            if 'ehpdev' in line:
                notransfer = os.path.join(event_dir, transfer.NO_TRANSFER)
                with open(notransfer, 'wt') as f:
                    f.write('transfer blocked by sm_migrate.')


def parse_config(event_dir, tarball, member, migrate_conf=None):
    errors = []
    conversions = {}
    if 'shake.conf' in member:
        pass
    elif 'grind_zc2.conf' in member:
        pass
    elif 'grind.conf' in member:
        pass
    elif 'db.conf' in member:
        pass
    elif 'timezone.conf':
        pass
    elif 'retrieve.conf' in member:
        pass
    elif 'transfer.conf' in member:
        parse_transfer(event_dir, tarball, member)
    else:
        errors.append('Unknown config file type %s' % member)

    return (errors, conversions)


def parse_info(event_dir, tarball, member, netid, networks,
               module_conf, skip_bounds=False, skip_settings=False, migrate_conf=None):
    jsonfile = tarball.extractfile(member)
    jsonstring = jsonfile.read().decode('utf-8')
    jsondict = json.loads(jsonstring)

    # accumulate error messages
    errors = []

    # accumulate gmpe conversions
    conversions = {}

    # get the origin information out of the json
    xmlfile = os.path.join(event_dir, 'event.xml')
    edict = {}
    edict['id'] = jsondict['input']['event_information']['event_id']
    timestring = jsondict['input']['event_information']['origin_time']
    edict['time'] = HistoricTime.strptime(timestring, TIMEFMT)
    edict['netid'] = netid
    if netid in networks:
        edict['network'] = networks[netid]
    else:
        network = get_network_name(netid)
        if network == 'unknown':
            network = ''
        networks[netid] = network
        edict['network'] = network
    edict['lat'] = jsondict['input']['event_information']['latitude']
    edict['lon'] = jsondict['input']['event_information']['longitude']
    edict['depth'] = jsondict['input']['event_information']['depth']
    edict['mag'] = jsondict['input']['event_information']['magnitude']
    edict['locstring'] = jsondict['input']['event_information']['location']
    edict['mech'] = jsondict['input']['event_information']['src_mech']
    write_event_file(edict, xmlfile)

    # get the above info as an amps.db event dictionary
    eqdict = {'id': edict['id'],
              'netid': edict['netid'],
              'network': edict['network'],
              'time': timestring[0:19]+'.000Z',
              'lat': edict['lat'],
              'lon': edict['lon'],
              'depth': edict['depth'],
              'mag': edict['mag'],
              'locstring': edict['locstring']}

    model = ConfigObj()
    if not skip_settings:
        # get all of the bias information
        model['modeling'] = {'bias': {}}
        bias_max_mag = jsondict['processing']['miscellaneous']['bias_max_mag']
        max_range = jsondict['processing']['miscellaneous']['bias_max_range']
        if bias_max_mag > 0 and max_range > 0:
            model['modeling']['bias']['do_bias'] = True
            model['modeling']['bias']['max_range'] = max_range
            model['modeling']['bias']['max_mag'] = bias_max_mag
        else:
            model['modeling']['bias']['do_bias'] = False

        # get the outlier information
        model['data'] = {'outlier': {}}
        max_deviation = jsondict['processing']['miscellaneous']['outlier_deviation_level']
        outlier_max_mag = jsondict['processing']['miscellaneous']['outlier_max_mag']
        if outlier_max_mag > 0 and max_deviation > 0:
            model['data']['outlier']['do_outlier'] = True
            model['data']['outlier']['max_deviation'] = max_deviation
            model['data']['outlier']['max_mag'] = outlier_max_mag
        else:
            model['data']['outlier']['do_outlier'] = False

        # get the gmice information
        allowed_gmice = module_conf['gmice_modules'].keys()
        gmice = jsondict['processing']['ground_motion_modules']['mi2pgm']['module']
        # WGRW11 in SM3.5 is WGRW12 in SM4.0
        if gmice == 'WGRW11':
            gmice = 'WGRW12'
        if gmice not in allowed_gmice:
            errors.append('GMICE %s (event %s) not yet supported in ShakeMap 4.' % (
                gmice, edict['id']))
        else:
            model['modeling']['gmice'] = gmice

        # get the GMPE information
        gmm_dict = jsondict['processing']['ground_motion_modules']
        old_gmpe = gmm_dict['gmpe']['module']
        new_gmpe, reference = migrate_gmpe(old_gmpe, config=migrate_conf)
        model = set_gmpe(new_gmpe, model, edict['id'])
        if old_gmpe not in conversions:
            conversions[old_gmpe] = (new_gmpe,reference)
        

    # work on map extent/resolution data
    if not skip_bounds:
        model['interp'] = {'prediction_location': {}}
        yres_km = jsondict['output']['map_information']['grid_spacing']['latitude']
        yres_sec = int(round(yres_km * KM2SEC))
        model['interp']['prediction_location']['xres'] = '%ic' % yres_sec
        model['interp']['prediction_location']['yres'] = '%ic' % yres_sec

        model['extent'] = {'bounds': {}}
        xmin = jsondict['output']['map_information']['min']['longitude']
        xmax = jsondict['output']['map_information']['max']['longitude']
        ymin = jsondict['output']['map_information']['min']['latitude']
        ymax = jsondict['output']['map_information']['max']['latitude']
        model['extent']['bounds']['extent'] = [xmin, ymin, xmax, ymax]

    # done with model.conf, merge with existing file and save
    if len(model):
        model_file = os.path.join(event_dir, 'model.conf')
        if os.path.isfile(model_file):
            existing_model = ConfigObj(model_file)
            model.merge(existing_model)
        model.filename = model_file
        model.write()

    return (eqdict, errors, conversions)


def create_event_directory(eventid, data_path):
    event_dir = os.path.join(data_path, eventid, 'current')
    if os.path.isdir(event_dir):
        return None
    os.makedirs(event_dir)
    return event_dir


if __name__ == '__main__':
    parser = get_parser()
    pargs, unknown = parser.parse_known_args()
    main(pargs)
